name: Attention

num_ner_segments: 40

feature_dim: 16
num_attention_layers: 6
n_head: 8
num_linear_layers: 1
linear_layer_size: 512

epochs: 50
batch_size: 128
learning_rate: 1e-3

print_interval: 100
eval_interval: 200
eval_samples: 3200

load_ner_features: True
save_ner_features: False

